# -*- coding: utf-8 -*-
"""Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/127Bonc7IOGCUim288LNBKhVtUbsEIpJu
"""

# STEP 1: Import libraries
import pandas as pd
import numpy as np
from math import sqrt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, classification_report,
    mean_absolute_error, mean_squared_error, r2_score
)
from sklearn.linear_model import Ridge, Lasso, ElasticNet, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.neural_network import MLPClassifier, MLPRegressor
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from xgboost import XGBClassifier, XGBRegressor
import joblib

# STEP 2: Upload and load dataset
df = pd.read_csv("Insurance Premium Prediction.csv")

# STEP 3: Map Exercise.Frequency to numeric
exercise_map = {'Rarely': 0, 'Monthly': 1, 'Weekly': 2, 'Daily': 3}
df['Exercise.Frequency'] = df['Exercise.Frequency'].map(exercise_map)

# STEP 4: Define features and targets
features = ['Annual.Income', 'Smoking.Status_Yes', 'Exercise.Frequency',
            'Health.Score', 'Vehicle.Age', 'Credit.Score', 'Insurance.Duration',
            'Number.of.Dependents', 'Property_Type_Code']

target_premium_range = 'premium_range_encoded'
target_policy = 'Policy_Type_Encoded'
target_vault = 'No_Claim_Vault_Eligibility'

# STEP 5: Remove outliers using IQR
Q1 = df[features].quantile(0.25)
Q3 = df[features].quantile(0.75)
IQR = Q3 - Q1
condition = ~((df[features] < (Q1 - 1.5 * IQR)) | (df[features] > (Q3 + 1.5 * IQR))).any(axis=1)
df = df[condition]

# STEP 6: Feature scaling
scaler = StandardScaler()
df[features] = scaler.fit_transform(df[features])


# STEP 7: Classification training function
def train_classification_models(X_train, X_test, y_train, y_test):
    models = {
        "Logistic Regression (L2)": LogisticRegressionCV(cv=5, penalty='l2', max_iter=1000),
        "Random Forest": RandomForestClassifier(),
        "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),
        "MLP": MLPClassifier(max_iter=1000),
        "Decision Tree": DecisionTreeClassifier(),
        "KNN": KNeighborsClassifier()
    }

    results = {}
    for name, model in models.items():
        model.fit(X_train, y_train)
        preds = model.predict(X_test)

        accuracy = accuracy_score(y_test, preds)
        precision = precision_score(y_test, preds, average='weighted', zero_division=0)
        recall = recall_score(y_test, preds, average='weighted', zero_division=0)
        f1 = f1_score(y_test, preds, average='weighted', zero_division=0)

        print(f"\nðŸ”¹ {name} Classification Report:")
        print(classification_report(y_test, preds, zero_division=0))

        results[name] = {
            'model': model,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1
        }
    return results


# STEP 8: Premium Range Prediction (Classification)
print("Premium Range Prediction (Classification):")
X = df[features]
y = df[target_premium_range]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
premium_range_results = train_classification_models(X_train, X_test, y_train, y_test)
best_premium_range = max(premium_range_results.items(), key=lambda x: x[1]['f1'])
joblib.dump(best_premium_range[1]['model'], 'best_premium_range_model.pkl')
print(f"\n Best Premium Range Model: {best_premium_range[0]}")

# STEP 9: Policy Type Prediction (Classification)
print("\n Policy Type Prediction (Classification):")
y = df[target_policy]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
policy_results = train_classification_models(X_train, X_test, y_train, y_test)
best_policy = max(policy_results.items(), key=lambda x: x[1]['f1'])
joblib.dump(best_policy[1]['model'], 'best_policy_model.pkl')
print(f"\n Best Policy Model: {best_policy[0]}")

# STEP 10: No-Claim Vault Eligibility Prediction (Classification)
print("\n No-Claim Vault Eligibility Prediction (Classification):")
y = df[target_vault]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
vault_results = train_classification_models(X_train, X_test, y_train, y_test)
best_vault = max(vault_results.items(), key=lambda x: x[1]['f1'])
joblib.dump(best_vault[1]['model'], 'best_vault_model.pkl')
print(f"\n Best Vault Model: {best_vault[0]}")

import joblib
import time
import numpy as np

from sklearn.model_selection import RandomizedSearchCV, train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier

# Define parameter grids
param_grid = {
    "KNN": {
        "n_neighbors": list(range(3, 21)),
        "weights": ["uniform", "distance"],
        "p": [1, 2]  # 1 = Manhattan, 2 = Euclidean
    },
    "Decision Tree": {
        "criterion": ["gini", "entropy"],
        "max_depth": [5, 10, 15, None],
        "min_samples_split": [2, 5, 10],
        "min_samples_leaf": [1, 2, 5]
    },
    "Random Forest": {
        "n_estimators": [100, 200],
        "max_depth": [10, 20, None],
        "min_samples_split": [2, 5],
        "min_samples_leaf": [1, 2]
    }
}

# Define models
models = {
    "KNN": KNeighborsClassifier(),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier()
}

# Define feature set
X = df[features]

# Define targets
y_premium = df[target_premium_range]
y_policy = df[target_policy]
y_vault = df[target_vault]

# Split datasets
X_train_premium, X_test_premium, y_train_premium, y_test_premium = train_test_split(X, y_premium, test_size=0.2, random_state=42)
X_train_policy, X_test_policy, y_train_policy, y_test_policy = train_test_split(X, y_policy, test_size=0.2, random_state=42)
X_train_vault, X_test_vault, y_train_vault, y_test_vault = train_test_split(X, y_vault, test_size=0.2, random_state=42)

# Map dataset to models
dataset_map = {
    "KNN": (X_train_premium, y_train_premium, X_test_premium, y_test_premium),
    "Decision Tree": (X_train_policy, y_train_policy, X_test_policy, y_test_policy),
    "Random Forest": (X_train_vault, y_train_vault, X_test_vault, y_test_vault)
}

# Scoring method (for classification)
scoring_map = {
    "KNN": "f1_weighted",
    "Decision Tree": "f1_weighted",
    "Random Forest": "f1_weighted"
}

best_models = {}

# Tuning Loop
for model_name, model in models.items():
    print(f"\nðŸ”§ Tuning {model_name}...")

    X_train, y_train, X_test, y_test = dataset_map[model_name]

    start_time = time.time()

    search = RandomizedSearchCV(
        estimator=model,
        param_distributions=param_grid[model_name],
        n_iter=10,
        scoring=scoring_map[model_name],
        cv=3,
        n_jobs=-1,
        verbose=1,
        random_state=42
    )

    search.fit(X_train, y_train)
    best_model = search.best_estimator_
    best_models[model_name] = best_model

    print(f"\n Best Parameters for {model_name}: {search.best_params_}")
    print(f" Best Cross-Validation Score: {search.best_score_:.4f}")
    print(f" Time Taken: {time.time() - start_time:.2f} seconds")

    # Evaluation
    y_pred = best_model.predict(X_test)

    print(f"\n Evaluation Metrics for {model_name}:")
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

    print(f"  Accuracy:  {accuracy:.4f}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall:    {recall:.4f}")
    print(f"  F1 Score:  {f1:.4f}")

    # Save the best model
    model_filename = f"{model_name.replace(' ', '_').lower()}_model.pkl"
    joblib.dump(best_model, model_filename)
    print(f" Saved {model_name} to '{model_filename}'")

import streamlit as st
import joblib
import numpy as np
import pandas as pd

# Load models
knn_model = joblib.load("knn_model.pkl")
decision_tree_model = joblib.load("decision_tree_model.pkl")
random_forest_model = joblib.load("random_forest_model.pkl")

# Define input features in the correct training order
feature_names = [
    'Annual.Income', 'Smoking.Status_Yes', 'Exercise.Frequency',
    'Health.Score', 'Vehicle.Age', 'Credit.Score', 'Insurance.Duration',
    'Number.of.Dependents', 'Property_Type_Code'
]

st.title("ðŸ”® Insurance Genie")

st.markdown("Fill in the attributes below to get predictions for:")
st.markdown("-  Recommended Premium Range")
st.markdown("-  Best-fit Policy Type")
st.markdown("-  Vault Eligibility")

# Input form
with st.form("prediction_form"):
    income = st.number_input("Annual Income", min_value=0.0, step=100.0)

    # Smoking Status
    smoking = st.selectbox("Do you Smoke?", options=["No", "Yes"])
    smoking_status = 1 if smoking == "Yes" else 0

    # Exercise Frequency (times per week)
    exercise_frequency = st.slider("Exercise Frequency (times per week)", min_value=0, max_value=14, step=1)

    health_score = st.slider("Health Score", min_value=0, max_value=100, step=1)
    vehicle_age = st.number_input("Vehicle Age (in years)", min_value=0.0, step=0.1)
    credit_score = st.slider("Credit Score", min_value=300, max_value=850, step=1)
    insurance_duration = st.slider("Insurance Duration (Years)", min_value=1, max_value=9, step=1)
    dependents = st.number_input("Number of Dependents", min_value=0, step=1)
    property_type = st.selectbox("Property Type Code", options=[1, 2, 3], help="Example: 1 = Apartment, 2 = Condo, 3 = House")

    submitted = st.form_submit_button("Predict")

if submitted:
    # Prepare input in the correct feature order
    input_data = np.array([[income, smoking_status, exercise_frequency,
                            health_score, vehicle_age, credit_score, insurance_duration,
                            dependents, property_type]])

    df_input = pd.DataFrame(input_data, columns=feature_names)

    # Predict Premium Range
    premium_pred = knn_model.predict(df_input)[0]

    # Predict Policy Type
    policy_pred = decision_tree_model.predict(df_input)[0]

    # Predict Vault Eligibility
    vault_pred = random_forest_model.predict(df_input)[0]
    vault_result = "Yes" if vault_pred == 1 else "No"

    # Display Results
    st.success(f" **Recommended Premium Range**: {premium_pred}")
    st.info(f"ðŸ“„ **Policy Type**: {policy_pred}")
    st.warning(f" **Vault Eligible**: {vault_result}")
